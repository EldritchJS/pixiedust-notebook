{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixiedust on Openshift/Daikon\n",
    "\n",
    "Start a pod via oc new-app pixiedust-notebook[-2.0.2].yaml\n",
    "Then browse to the route created as a result\n",
    "\n",
    "# Initial Pixiedust Import\n",
    "\n",
    "The first time pixiedust is imported within a Jupyter pyspark instance after installation (or pod instantiation) a kernel restart is requested. If none is requested, move past this cell without a restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import\n",
    "\n",
    "Pixiedust has a number of import methods in its API which return a Spark DataFrame. In addition, it has a number of sample data sets to provide a quick start to data science using Pixiedust. Its visualization tools do not require using its data import tools, allowing legacy DataFrame code to remain intact.\n",
    "\n",
    "```python\n",
    "# Standard approaches\n",
    "sqlContext.createDataFrame(<Data values>)\n",
    "spark.read()\n",
    "# Additional Pixiedust approaches\n",
    "pixiedust.sampleData(<URL>)\n",
    "pixiedust.sampleData({1-7})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Package Install/Management\n",
    "Pixiedust advertises a means for installing Spark packages with lower overhead. Some things to note: the example code in some of the Pixiedust sample notebooks is flawed, as it omits a key step. The flow is as follows:\n",
    "1 Use pixiedust.installPackage({spark-package.org string, Maven repo info, URL to Jar}) to install the package/jar\n",
    "2 Restart the notebook kernel\n",
    "3 import pixiedust\n",
    "4 Use installPackage() to load the installed package\n",
    "Note: This has mixed results. For instance, the GraphFrames example Pixiedust uses does not function even after loading properly. The following example, however, does function (as of 12 June 2017 at least)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixiedust.installPackage(\"TargetHolding:pyspark-cassandra:0.3.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixiedust.printAllPackages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pixiedust\n",
    "import pyspark_cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_cassandra.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Job Monitor\n",
    "\n",
    "Pixiedust advertises a built-in spark job monitor for displaying job progress _in situ_ rather than tailing logfiles or otherwise. This is a neat feature, however, it's the only one which absolutely **does not** work beyond Spark 2.0. While the monitor can be enabled irrespective of version, beyond 2.0 an ugly error message will appear after a successfully enabled message(sigh), but otherwise the rest of the notebook should run fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixiedust.enableJobMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = pixiedust.sampleData(\"https://github.com/ibm-cds-labs/open-data/raw/master/cars/cars.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "\n",
    "This is where Pixiedust truly shines. Much like Tableau, Pixiedust takes a bunch of the heavy lifting out of exploratory data analyses by providing a means for trying out different visualizations on a DataFrame. Indeed, one need merely call `display(<DataFrame>)` and be on their way. No visualization lib imports, no advanced settings, just call and run with it.\n",
    "\n",
    "## Visualizing a Two-column DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tutorial paste: create a Spark dataframe, passing in some data, and assign it to a variable \n",
    "df = sqlContext.createDataFrame(\n",
    "[(\"Black\", 87),\n",
    " (\"Red\", 13)],\n",
    "[\"Colors\",\"%\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "dataframe"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing an N-column DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {
      "handlerId": "dataframe"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Slightly modified Tutorial paste\n",
    "df2 = sqlContext.createDataFrame(\n",
    "[(2010, 'Air Hockey', 10),\n",
    " (2010, 'Curling', 20),\n",
    " (2010, 'Kendo', 1),\n",
    " (2010, 'Iaido', 2),\n",
    " (2010, 'Ninjitsu', 1),\n",
    " (2010, 'Ping Pong', 50),\n",
    " (2011, 'Air Hockey', 15),\n",
    " (2011, 'Curling', 30),\n",
    " (2011, 'Kendo', 5),\n",
    " (2011, 'Iaido', 10),\n",
    " (2011, 'Ninjitsu', 2),\n",
    " (2011, 'Ping Pong', 45),\n",
    " (2012, 'Air Hockey', 19),\n",
    " (2012, 'Curling', 34),\n",
    " (2012, 'Kendo', 6),\n",
    " (2012, 'Iaido', 11),\n",
    " (2012, 'Ninjitsu', 3),\n",
    " (2012, 'Ping Pong', 40)],\n",
    "[\"year\",\"sport\",\"unique_fans\"])\n",
    "\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sample Pixiedust data set to explore car data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {
      "handlerId": "dataframe"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Another tutorial paste, interesting cars data set\n",
    "df3 = pixiedust.sampleData(\"https://github.com/ibm-cds-labs/open-data/raw/master/cars/cars.csv\")\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sample Pixiedust data set to explore Boston crime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "basemap": "satellite-v9",
      "handlerId": "mapView",
      "keyFields": "X,Y",
      "kind": "simple",
      "mapboxtoken": "pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4M29iazA2Z2gycXA4N2pmbDZmangifQ.-g_vE53SD2WrJ6tFX7QHmA",
      "rendererId": "mapbox",
      "rowCount": "500",
      "valueFields": "type"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Not a tutorial paste, fun dataset\n",
    "bostonCrime = pixiedust.sampleData(7)\n",
    "display(bostonCrime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scala Bridge\n",
    "\n",
    "Pixiedust provides a means for sharing Python variables/data with Scala and Scala with Python. Scala code is entered after issueing the Pixiedust magic %%scala. \n",
    "\n",
    "## Python ---> Scala\n",
    "In the below example borrowed from a tutorial, Python variables are created then Scala is entered and the Python variables are printed. For Strings, they must be defined using double quotes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dog=\"Weechee\"\n",
    "person=\"Jason\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%scala\n",
    "println(s\"$person has a dog named $dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scala ---> Python\n",
    "\n",
    "In this example, a DataFrame is created within the Scala magic and the resulting DataFrame is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%scala\n",
    "// Slightly modified Tutorial paste\n",
    "//Reuse the sqlContext object available in the python scope\n",
    "val c = sqlContext.asInstanceOf[org.apache.spark.sql.SQLContext]\n",
    "import c.implicits._\n",
    "\n",
    "val __dfFromScala = Seq(\n",
    "    (2010, \"Air Hockey\", 10),\n",
    "    (2010, \"Curling\", 20),\n",
    "    (2010, \"Kendo\", 1),\n",
    "    (2010, \"Iaido\", 2),\n",
    "    (2010, \"Ninjitsu\", 1),\n",
    "    (2010, \"Ping Pong\", 50),\n",
    "    (2011, \"Air Hockey\", 15),\n",
    "    (2011, \"Curling\", 30),\n",
    "    (2011, \"Kendo\", 5),\n",
    "    (2011, \"Iaido\", 10),\n",
    "    (2011, \"Ninjitsu\", 2),\n",
    "    (2011, \"Ping Pong\", 45),\n",
    "    (2012, \"Air Hockey\", 19),\n",
    "    (2012, \"Curling\", 34),\n",
    "    (2012, \"Kendo\", 6),\n",
    "    (2012, \"Iaido\", 11),\n",
    "    (2012, \"Ninjitsu\", 3),\n",
    "    (2012, \"Ping Pong\", 40)).toDF(\"year\",\"sport\",\"unique_fans\")\n",
    "     \n",
    "__dfFromScala.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the DataFrame from Scala is converted to a Python DataFrame. Note: The Pixiedust tutorials do not include the conversion, however, it appears necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.common import _py2java, _java2py\n",
    "\n",
    "pythonDF = _java2py(sc, __dfFromScala)\n",
    "\n",
    "display(pythonDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python with Pixiedust (Spark 2.2)",
   "language": "python",
   "name": "pythonwithpixiedustspark22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
