{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixiedust on Openshift/Daikon\n",
    "\n",
    "Image on Dockerhub: eldritchjs/pixiedust-notebook\n",
    "\n",
    "# Initial Pixiedust Import\n",
    "\n",
    "The first time pixiedust is imported within a Jupyter pyspark instance after installation (or pod instantiation) a kernel restart is requested. If none is requested, move past this cell without a restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import\n",
    "\n",
    "Pixiedust has a number of import methods in its API which return a Spark DataFrame. In addition, it has a number of sample data sets to provide a quick start to data science using Pixiedust. Its visualization tools do not require using its data import tools, allowing legacy DataFrame code to remain intact.\n",
    "\n",
    "```python\n",
    "# Standard approaches\n",
    "sqlContext.createDataFrame(<Data values>)\n",
    "spark.read()\n",
    "# Additional Pixiedust approaches\n",
    "pixiedust.sampleData(<URL>)\n",
    "pixiedust.sampleData({1-7})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Package Install/Management\n",
    "Pixiedust advertises a means for installing Spark packages with lower overhead. Some things to note: the example code in some of the Pixiedust sample notebooks is flawed, as it omits a key step. The flow is as follows:\n",
    "1 Use pixiedust.installPackage({spark-package.org string, Maven repo info, URL to Jar}) to install the package/jar\n",
    "2 Restart the notebook kernel\n",
    "3 import pixiedust\n",
    "4 Use installPackage() to load the installed package\n",
    "Note: This has mixed results. For instance, the GraphFrames example Pixiedust uses does not function even after loading properly. The following example, however, does function (as of 12 June 2017 at least)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixiedust.installPackage(\"TargetHolding:pyspark-cassandra:0.3.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixiedust.printAllPackages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust\n",
    "import pyspark_cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_cassandra.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Job Monitor\n",
    "\n",
    "Pixiedust advertises a built-in spark job monitor for displaying job progress _in situ_ rather than tailing logfiles or otherwise. This is a neat feature, however, it's the only one which absolutely **does not** work beyond Spark 2.0. While the monitor can be enabled irrespective of version, beyond 2.0 an ugly error message will appear after a successfully enabled message(sigh), but otherwise the rest of the notebook should run fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixiedust.enableJobMonitor()\n",
    "\n",
    "df3 = pixiedust.sampleData(\"https://github.com/ibm-cds-labs/open-data/raw/master/cars/cars.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "\n",
    "This is where Pixiedust truly shines. Much like Tableau, Pixiedust takes a bunch of the heavy lifting out of exploratory data analyses by providing a means for trying out different visualizations on a DataFrame. Indeed, one need merely call display(<DataFrame>) and be on their way. No visualization lib imports, no advanced settings, just call and run with it.\n",
    "\n",
    "## Two-column DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial paste: create a Spark dataframe, passing in some data, and assign it to a variable \n",
    "df = sqlContext.createDataFrame(\n",
    "[(\"Black\", 87),\n",
    " (\"Red\", 13)],\n",
    "[\"Colors\",\"%\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "dataframe"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N -column DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "dataframe"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Slightly modified Tutorial paste\n",
    "df2 = sqlContext.createDataFrame(\n",
    "[(2010, 'Air Hockey', 10),\n",
    " (2010, 'Curling', 20),\n",
    " (2010, 'Kendo', 1),\n",
    " (2010, 'Iaido', 2),\n",
    " (2010, 'Ninjitsu', 1),\n",
    " (2010, 'Ping Pong', 50),\n",
    " (2011, 'Air Hockey', 15),\n",
    " (2011, 'Curling', 30),\n",
    " (2011, 'Kendo', 5),\n",
    " (2011, 'Iaido', 10),\n",
    " (2011, 'Ninjitsu', 2),\n",
    " (2011, 'Ping Pong', 45),\n",
    " (2012, 'Air Hockey', 19),\n",
    " (2012, 'Curling', 34),\n",
    " (2012, 'Kendo', 6),\n",
    " (2012, 'Iaido', 11),\n",
    " (2012, 'Ninjitsu', 3),\n",
    " (2012, 'Ping Pong', 40)],\n",
    "[\"year\",\"sport\",\"unique_fans\"])\n",
    "\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sample Pixiedust data set to explore car data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "dataframe"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Another tutorial paste, interesting cars data set\n",
    "df3 = pixiedust.sampleData(\"https://github.com/ibm-cds-labs/open-data/raw/master/cars/cars.csv\")\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sample Pixiedust data set to explore Boston crime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "basemap": "satellite-v9",
      "handlerId": "mapView",
      "keyFields": "X,Y",
      "kind": "simple",
      "mapboxtoken": "pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4M29iazA2Z2gycXA4N2pmbDZmangifQ.-g_vE53SD2WrJ6tFX7QHmA",
      "rendererId": "mapbox",
      "rowCount": "500",
      "valueFields": "type"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Not a tutorial paste, fun dataset\n",
    "bostonCrime = pixiedust.sampleData(7)\n",
    "display(bostonCrime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python with Pixiedust (Spark 2.1)",
   "language": "python",
   "name": "pythonwithpixiedustspark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
